{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning session\n",
    "<em>Instructor:</em> Soufiane Mourragui\n",
    "<br/>\n",
    "<em>Contact: </em>s.mourragui@nki.nl & @souf_mourra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Standard machine learning approaches rely on the idea that the data used for prediction (<em>test</em> or <em>evaluation</em> set) comes from the same distribution as the data used for training the model. Unfortunately, <b>this is rarely the case</b> and predictive models are often trained on a dataset that significantly differs from the dataset used for prediction (e.g. in the clinic).\n",
    "<br/>\n",
    "Such behaviors has already led to several failures in predictive modelling:\n",
    "<ul>\n",
    "    <li> Amazon's program to automate CV pre-screening process turned out to favor male profiles over women (2018, <a href=\"https://www.bbc.com/news/technology-45809919\">source</a>).\n",
    "    <li> Face-recognition programs struggles to recognize minorities (2019, <a href=\"https://www.wired.com/story/best-algorithms-struggle-recognize-black-faces-equally/\">source</a>).\n",
    "    <li> On a lighter note: AI systems for ball-tracking confuses the ball with the referee's head (2020, <a href=\"https://www.diyphotography.net/ai-camera-thinks-bald-referees-head-is-a-soccer-ball/\">source</a>)\n",
    "</ul>\n",
    "There is therefore a need to correct the source (training) and target (test) datasets to help the predictive model focus on the generalizable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different approaches\n",
    "Different approaches exist for solving this problem, see <a href=\"https://ieeexplore.ieee.org/abstract/document/8861136\">[Kouw et al 2019]</a> for a comprehensive review. Roughly speaking, these can be divided into the following categories, depending on data-availabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./images/transfer_learning.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Inductive transfer learning</b> corresponds to the case where the source and target data comes from the same distribution (e.g. images from the streets of Amsterdam on a sunny day) but the prediction task is different (e.g. in source we predict the number of cars, in target the number of bikes).\n",
    "<br/><br/>\n",
    "<b>Transductive transfer learning</b> or <b>domain adaptation</b> corresponds to the case where the task is the same (e.g. in both source and target we want to predict the number of cars in the stret) but the source and target data comes from different images (e.g. images from Amsterdam in source, and from Paris in target).\n",
    "<br/><br/>\n",
    "<b>Unsupervised transfer learning</b> finally corresponds to the case where the task and the domains are different. This is the most challenging ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here focus on <b>domain adaptation</b> in the case of drug response prediction in cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: predicting drug response in patient derived xenografts (PDXs) from cell line models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell line models are versatile and useful models to study the mechanisms of drug response. However, they do not faithfully mimic the biological processes found in-vivo which makes the translation of findings difficult. We here study the drug response prediction problem and try to predict the response in patient derived xenografts (PDXs) using as training data the drug response of cell lines. We will focus on Gemcitabine.\n",
    "<br/><br/>\n",
    "<b>Data</b>\n",
    "- GDSC for cell lines, with drug response measured as AUC (Area Under the drug response Curve).\n",
    "- PDXE for the patient derived xenografts, with drug response measured as Best Average Response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from transact.TRANSACT import TRANSACT\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context('paper')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "### Gene expression (in the forms of FPKMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDSC_file = '../data/FPKM_all_GDSC.csv'\n",
    "GDSC_FPKM_df = pd.read_csv(GDSC_file, index_col=0)\n",
    "\n",
    "PDXE_file = '../data/FPKM_all_PDXE.csv'\n",
    "PDXE_FPKM_df = pd.read_csv(PDXE_file, index_col=0)\n",
    "\n",
    "GDSC_FPKM_df = pd.DataFrame(StandardScaler(with_mean=True, with_std=True).fit_transform(GDSC_FPKM_df),\n",
    "                            index=GDSC_FPKM_df.index, \n",
    "                            columns=GDSC_FPKM_df.columns)\n",
    "PDXE_FPKM_df = pd.DataFrame(StandardScaler(with_mean=True, with_std=True).fit_transform(PDXE_FPKM_df),\n",
    "                            index=PDXE_FPKM_df.index, \n",
    "                            columns=PDXE_FPKM_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug response\n",
    "Drug response is measured as AUC for cell lines (GDSC) and as Best Average Response for patient derived xenografts (PDXs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDSC_response_file = '../data/response_GDSC_Gemcitabine.csv'\n",
    "GDSC_response_df = pd.read_csv(GDSC_response_file, index_col=0)\n",
    "\n",
    "PDXE_response_file = '../data/response_PDXE_gemcitabine-50mpk.csv'\n",
    "PDXE_response_df = pd.read_csv(PDXE_response_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDSC_FPKM_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without domain adaptation\n",
    "We here train a deep neural network on cell lines and predict the response in PDXs. We propose this architecture after a 5-fold cross-validation on GDSC to select the neural network with the largest expressivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on GDSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dropout = 0.1\n",
    "hidden_dropout = 0.5\n",
    "weight_decay = 0.0001\n",
    "learning_rate = 0.0005\n",
    "activation = torch.nn.Tanh\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 16\n",
    "n_epochs = 100\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(input_dropout),\n",
    "    torch.nn.Linear(GDSC_FPKM_df.shape[1], 252),\n",
    "    activation(),\n",
    "    torch.nn.Dropout(hidden_dropout),\n",
    "    torch.nn.Linear(252, 252),\n",
    "    activation(),\n",
    "    torch.nn.Dropout(hidden_dropout),\n",
    "    torch.nn.Linear(252, 1)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDSC_response_samples = GDSC_response_df.index.get_level_values(0)\n",
    "X = GDSC_FPKM_df.loc[GDSC_response_samples].values.astype(np.float32)\n",
    "y = GDSC_response_df.loc[GDSC_response_samples]['AUC'].values.astype(np.float32)\n",
    "data = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "train_loader = DataLoader(dataset=data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on whole dataset\n",
    "loss_values_train = []\n",
    "\n",
    "rescale_output = lambda x: x.detach().numpy().flatten()\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "    # Training\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        net.train()\n",
    "        y_predict_train = net(x_batch)\n",
    "        loss = loss_func(y_predict_train, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "        loss_values_train.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "PDXE_response_samples = PDXE_response_df.index.get_level_values(0)\n",
    "X_target = torch.Tensor(PDXE_FPKM_df.loc[PDXE_response_samples].values)\n",
    "PDXE_non_adapted_prediction = net(X_target).detach().numpy().flatten()\n",
    "PDXE_uncorrected_corr = scipy.stats.spearmanr(PDXE_non_adapted_prediction,\n",
    "                                              PDXE_response_df.values.flatten())\n",
    "print(PDXE_uncorrected_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-adaptation\n",
    "We will here use TRANSACT to correct the signal before feeding it into a complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "kernel_params = {'gamma': 10**(-3.5)}\n",
    "\n",
    "n_components = {\n",
    "    'source': 70,\n",
    "    'target': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSACT_clf = TRANSACT(kernel=kernel,\n",
    "                        kernel_params=kernel_params,\n",
    "                        n_components=n_components,\n",
    "                        n_jobs=30, \n",
    "                        verbose=1)\n",
    "TRANSACT_clf.fit(GDSC_FPKM_df.values, PDXE_FPKM_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity matrix\n",
    "To grasp the difference between cell-lines and tumors, let's visualise the similarity between cell lines Non Linear Principal Components (NLPCs) and the patient derived xenografts NLPCs. This is quantified by the cosine similarity matrix that compares each cell line NLPCs to each PDX NLPCs. In an ideal scenario where the two domains match, we would have a diagonal matrix, suggesting that both datasets are supported by the same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(TRANSACT_clf.principal_vectors_.cosine_similarity_, cmap='seismic_r', center=0, vmax=1, vmin=-1)\n",
    "plt.ylabel('Cell lines NLPCs', fontsize=25, color='black')\n",
    "plt.xlabel('PDX NLPCs', fontsize=25, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of alignment\n",
    "We use the notion of principal vectors (PVs) to align these NLPCs and find pairs of vectors (one from source, one from target) as close as possible to one another. The cosine similarity matrix must be more diagonal then ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15,6))\n",
    "\n",
    "# Cosine similarity\n",
    "sns.heatmap(np.diag(np.cos(TRANSACT_clf.principal_vectors_.canonical_angles)),\n",
    "            cmap='seismic_r', center=0, vmax=1, vmin=-1, ax=axes[0])\n",
    "axes[0].set_ylabel('Cell lines NLPCs', fontsize=25, color='black')\n",
    "axes[0].set_xlabel('PDX NLPCs', fontsize=25, color='black')\n",
    "\n",
    "axes[1].plot(np.cos(TRANSACT_clf.principal_vectors_.canonical_angles), marker='+')\n",
    "axes[1].set_ylabel('Similarity between PVs', fontsize=25, color='black')\n",
    "axes[1].set_xlabel('PV number', fontsize=25, color='black')\n",
    "axes[1].tick_params(axis='both', labelsize=15)\n",
    "axes[1].set_ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a predictor on cell lines\n",
    "We here selct the PVs with similarity above 0.5 ($n_{pv}\\approx 20$) and train a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSACT_clf.fit(GDSC_FPKM_df.values, PDXE_FPKM_df.values, n_pv=20, with_interpolation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSACT_clf.fit_predictor(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying it on PDXs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDXE_adapted_corr = scipy.stats.spearmanr(TRANSACT_clf.predict(X_target), PDXE_response_df.values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the two predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.bar(x=[0,1], height=[PDXE_uncorrected_corr[0], PDXE_adapted_prediction[0]])\n",
    "plt.xticks([0,1], ['Uncorrected', 'TRANSACT'], fontsize=20, color='black')\n",
    "plt.yticks(fontsize=15, color='black')\n",
    "plt.ylabel('Correlation on PDXs', fontsize=20, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we did not touch upon\n",
    "We showed an example of domain adaptation applied to genomics. There are various other applications and transfer learning methods that exist out there. A few interesting examples:\n",
    "<ul>\n",
    "    <li> <b>Neural network pre-training</b>: Once a neural network has been trained on a source dataset, it can be interesting to tune it on the target dataset. This only applies if some labelled data is available on the test set.\n",
    "    <li> <b>Sample reweighting</b>: Another strategy consists in weighting source samples according to how well they fit inside the target data. For instance, a source sample with a lot of close target samples will be down-weighted -- in the meantime, a source samples without any close correspondance in the target data will be down-weighted.\n",
    "    <li> <b>Domain adaptation theory</b>:\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- <a href=\"https://ieeexplore.ieee.org/abstract/document/8861136\">A review of domain adaptation without target labels </a>, Kouw et al 2019, TPAMI. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oncode_mc",
   "language": "python",
   "name": "oncode_mc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
