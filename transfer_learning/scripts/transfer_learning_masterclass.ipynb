{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning session\n",
    "<em>Instructor:</em> Soufiane Mourragui\n",
    "<br/>\n",
    "<em>Contact: </em>s.mourragui@nki.nl & @souf_mourra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Standard machine learning approaches rely on the idea that the data used for prediction (<em>test</em> or <em>evaluation</em> set) comes from the same distribution as the data used for training the model. Unfortunately, <b>this is rarely the case</b> and predictive models are often trained on a dataset that significantly differs from the dataset used for prediction (e.g. in the clinic).\n",
    "<br/>\n",
    "Such behaviors has already led to several failures in predictive modelling:\n",
    "<ul>\n",
    "    <li> Amazon's program to automate CV pre-screening process turned out to favor male profiles over women (2018, <a href=\"https://www.bbc.com/news/technology-45809919\">source</a>).\n",
    "    <li> Face-recognition programs struggles to recognize minorities (2019, <a href=\"https://www.wired.com/story/best-algorithms-struggle-recognize-black-faces-equally/\">source</a>).\n",
    "    <li> On a lighter note: AI systems for ball-tracking confused by bald referee's head (2020, <a href=\"https://www.diyphotography.net/ai-camera-thinks-bald-referees-head-is-a-soccer-ball/\">source</a>)\n",
    "</ul>\n",
    "There is therefore a need to correct the source (training) and target (test) datasets to help the predictive model focus on the generalizable information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different approaches\n",
    "Different approaches exist for solving this problem, see <a href=\"https://ieeexplore.ieee.org/abstract/document/8861136\">[Kouw et al 2019]</a> for a comprehensive review. Roughly speaking, these can be divided into the following categories, depending on data-availabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./images/transfer_learning.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Inductive transfer learning</b> corresponds to the case where the source and target data comes from the same distribution (e.g. images from the streets of Amsterdam on a sunny day) but the prediction task is different (e.g. in source we predict the number of cars, in target the number of bikes).\n",
    "<br/><br/>\n",
    "<b>Transductive transfer learning</b> or <b>domain adaptation</b> corresponds to the case where the task is the same (e.g. in both source and target we want to predict the number of cars in the stret) but the source and target data comes from different images (e.g. images from Amsterdam in source, and from Paris in target).\n",
    "<br/><br/>\n",
    "<b>Unsupervised transfer learning</b> finally corresponds to the case where the task and the domains are different. This is the most challenging ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will here focus on <b>domain adaptation</b> in the case of drug response prediction in cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: diving deeper into domain adaptation\n",
    "We consider to datasets: a source and a target dataset, represented respectively by $\\mathcal{X}_s \\times \\mathcal{Y}_s$ and $\\mathcal{X}_t \\times \\mathcal{Y}_t$. In domain adaptation we always have $P\\left(\\mathcal{X}_s,\\mathcal{Y}_s\\right) \\neq P\\left(\\mathcal{X}_t,\\mathcal{Y}_t\\right)$. The ways these distributions differ create markedly different scenarios with different methods:\n",
    "<ul>\n",
    "    <li> $P \\left(\\mathcal{X}_s\\right) = P \\left(\\mathcal{X}_t\\right)$ but $P \\left(\\mathcal{Y}_s|\\mathcal{X}_s\\right) \\neq P \\left(\\mathcal{Y}_s|\\mathcal{X}_s\\right)$. In this case, a generative model can be used to learn the marginal distribution and used on the target. However, some target labels will be required to train a model. \n",
    "        <br/><br/>\n",
    "    <li> $P \\left(\\mathcal{Y}_s|\\mathcal{X}_s\\right) = P \\left(\\mathcal{Y}_s|\\mathcal{X}_s\\right)$ but $P \\left(\\mathcal{X}_s\\right) \\neq P \\left(\\mathcal{X}_t\\right)$. This scenario is called <b>covariate shift</b>. In case when the discriminator (machine learning model) perfectly learns the conditional distribution, no correction is required. As it is in practice never the case, methods to correct for this scenario rely either on a re-sampling, a re-weighting, or a unsupervised correction.\n",
    "        <br/><br/>\n",
    "    <li> $P \\left(\\mathcal{Y}_s|\\mathcal{X}_s\\right) \\neq P \\left(\\mathcal{Y}_s|\\mathcal{X}_s\\right)$ and $P \\left(\\mathcal{X}_s\\right) \\neq P \\left(\\mathcal{X}_t\\right)$. In this case, additional hypothesis have to be added to the system and specific methods have been designed for this problem.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem: predicting drug response in patient derived xenografts (PDXs) from cell line models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cell line models are versatile and useful models to study the mechanisms of drug response. However, they do not faithfully mimic the biological processes found in-vivo which makes the translation of findings difficult. We here study the drug response prediction problem and try to predict the response in patient derived xenografts (PDXs) using as training data the drug response of cell lines. We will focus on Gemcitabine.\n",
    "<br/><br/>\n",
    "<b>Data</b>\n",
    "- GDSC for cell lines, with drug response measured as AUC (Area Under the drug response Curve).\n",
    "- PDXE for the patient derived xenografts, with drug response measured as Best Average Response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from transact.TRANSACT import TRANSACT\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context('paper')\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "### Gene expression (in the forms of FPKMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDSC_file = '../data/FPKM_all_GDSC.csv'\n",
    "GDSC_FPKM_df = pd.read_csv(GDSC_file, index_col=0)\n",
    "\n",
    "PDXE_file = '../data/FPKM_all_PDXE.csv'\n",
    "PDXE_FPKM_df = pd.read_csv(PDXE_file, index_col=0)\n",
    "\n",
    "GDSC_FPKM_df = pd.DataFrame(StandardScaler(with_mean=True, with_std=True).fit_transform(GDSC_FPKM_df),\n",
    "                            index=GDSC_FPKM_df.index, \n",
    "                            columns=GDSC_FPKM_df.columns)\n",
    "PDXE_FPKM_df = pd.DataFrame(StandardScaler(with_mean=True, with_std=True).fit_transform(PDXE_FPKM_df),\n",
    "                            index=PDXE_FPKM_df.index, \n",
    "                            columns=PDXE_FPKM_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drug response\n",
    "Drug response is measured as AUC for cell lines (GDSC) and as Best Average Response for patient derived xenografts (PDXs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDSC_response_file = '../data/response_GDSC_Gemcitabine.csv'\n",
    "GDSC_response_df = pd.read_csv(GDSC_response_file, index_col=0)\n",
    "\n",
    "PDXE_response_file = '../data/response_PDXE_gemcitabine-50mpk.csv'\n",
    "PDXE_response_df = pd.read_csv(PDXE_response_file, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without domain adaptation\n",
    "We here train a deep neural network on cell lines and predict the response in PDXs. We propose this architecture after a 5-fold cross-validation on GDSC to select the neural network with the largest expressivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on GDSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dropout = 0.1\n",
    "hidden_dropout = 0.5\n",
    "weight_decay = 0.0001\n",
    "learning_rate = 0.0005\n",
    "activation = torch.nn.Tanh\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 16\n",
    "n_epochs = 100\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(input_dropout),\n",
    "    torch.nn.Linear(GDSC_FPKM_df.shape[1], 252),\n",
    "    activation(),\n",
    "    torch.nn.Dropout(hidden_dropout),\n",
    "    torch.nn.Linear(252, 252),\n",
    "    activation(),\n",
    "    torch.nn.Dropout(hidden_dropout),\n",
    "    torch.nn.Linear(252, 1)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GDSC_response_samples = GDSC_response_df.index.get_level_values(0)\n",
    "X = GDSC_FPKM_df.loc[GDSC_response_samples].values.astype(np.float32)\n",
    "y = GDSC_response_df.loc[GDSC_response_samples]['AUC'].values.astype(np.float32)\n",
    "data = TensorDataset(torch.Tensor(X), torch.Tensor(y))\n",
    "train_loader = DataLoader(dataset=data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on whole dataset\n",
    "loss_values_train = []\n",
    "\n",
    "rescale_output = lambda x: x.detach().numpy().flatten()\n",
    "\n",
    "for epoch in range(n_epochs+1):\n",
    "    # Training\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        net.train()\n",
    "        y_predict_train = net(x_batch)\n",
    "        loss = loss_func(y_predict_train, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "        loss_values_train.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "PDXE_response_samples = PDXE_response_df.index.get_level_values(0)\n",
    "X_target = torch.Tensor(PDXE_FPKM_df.loc[PDXE_response_samples].values)\n",
    "PDXE_non_adapted_prediction = net(X_target).detach().numpy().flatten()\n",
    "PDXE_uncorrected_corr = scipy.stats.spearmanr(PDXE_non_adapted_prediction,\n",
    "                                              PDXE_response_df.values.flatten())\n",
    "print(PDXE_uncorrected_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-adaptation\n",
    "We will here use TRANSACT ([Mourragui et al 2020]) to correct the signal before feeding it into a complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = 'rbf'\n",
    "kernel_params = {'gamma': 5*10**(-4)}\n",
    "\n",
    "n_components = {\n",
    "    'source': 70,\n",
    "    'target': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSACT_clf = TRANSACT(kernel=kernel,\n",
    "                        kernel_params=kernel_params,\n",
    "                        n_components=n_components,\n",
    "                        n_jobs=30, \n",
    "                        verbose=1)\n",
    "TRANSACT_clf.fit(GDSC_FPKM_df.values, PDXE_FPKM_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity matrix\n",
    "To grasp the difference between cell-lines and tumors, let's visualise the similarity between cell lines Non Linear Principal Components (NLPCs) and the patient derived xenografts NLPCs. This is quantified by the cosine similarity matrix that compares each cell line NLPCs to each PDX NLPCs. In an ideal scenario where the two domains match, we would have a diagonal matrix, suggesting that both datasets are supported by the same variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,6))\n",
    "sns.heatmap(TRANSACT_clf.principal_vectors_.cosine_similarity_, cmap='seismic_r', center=0, vmax=1, vmin=-1)\n",
    "plt.ylabel('Cell lines NLPCs', fontsize=25, color='black')\n",
    "plt.xlabel('PDX NLPCs', fontsize=25, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of alignment\n",
    "We use the notion of principal vectors (PVs) to align these NLPCs and find pairs of vectors (one from source, one from target) as close as possible to one another. The cosine similarity matrix must be more diagonal then ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15,6))\n",
    "\n",
    "# Cosine similarity\n",
    "sns.heatmap(np.diag(np.cos(TRANSACT_clf.principal_vectors_.canonical_angles)),\n",
    "            cmap='seismic_r', center=0, vmax=1, vmin=-1, ax=axes[0])\n",
    "axes[0].set_ylabel('Cell lines NLPCs', fontsize=25, color='black')\n",
    "axes[0].set_xlabel('PDX NLPCs', fontsize=25, color='black')\n",
    "\n",
    "axes[1].plot(np.cos(TRANSACT_clf.principal_vectors_.canonical_angles), marker='+')\n",
    "axes[1].set_ylabel('Similarity between PVs', fontsize=25, color='black')\n",
    "axes[1].set_xlabel('PV number', fontsize=25, color='black')\n",
    "axes[1].tick_params(axis='both', labelsize=15)\n",
    "axes[1].set_ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a predictor on cell lines\n",
    "We here selct the PVs with similarity above 0.5 ($n_{pv}\\approx 20$) and train a predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSACT_clf.fit(GDSC_FPKM_df.values, PDXE_FPKM_df.values, n_pv=20, with_interpolation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSACT_clf.fit_predictor(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying it on PDXs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDXE_adapted_corr = scipy.stats.spearmanr(TRANSACT_clf.predict(X_target), PDXE_response_df.values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the two predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.bar(x=[0,1], height=[PDXE_uncorrected_corr[0], PDXE_adapted_corr[0]])\n",
    "plt.xticks([0,1], ['Uncorrected', 'TRANSACT'], fontsize=20, color='black')\n",
    "plt.yticks(fontsize=15, color='black')\n",
    "plt.ylabel('Correlation on PDXs', fontsize=20, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: repeating experiment on Erlotinib, or re-seeding neural network and re-running several times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: is the similarity significant ?\n",
    "We here shuffle randomly the genes in the source and re-apply TRANSACT to measure the resulting similarity. This would give us an idea of the structure that exists in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_sim = TRANSACT_clf.null_distribution_pv_similarity(n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, figsize=(10,6))\n",
    "\n",
    "plt.plot(np.arange(TRANSACT_clf.n_pv)+1, np.cos(TRANSACT_clf.principal_vectors_.canonical_angles), marker='+')\n",
    "plt.boxplot(null_sim)\n",
    "plt.ylabel('PV similarity', fontsize=25, color='black')\n",
    "plt.xlabel('PV rank', fontsize=25, color='black')\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why such an inflation in the first permuted PVs ? Hint: look at the Taylor expansion of the kernel !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Maximum mean discrepancy (MMD) to compare two \n",
    "The Maximum Mean Discrepancy (Gretton et al 2012) is a powerful way to compare two distributions. Source and target will be represented by their data-generating distributions $\\mu_s$ and $\\mu_t$, and their samples data $X_s$ and $X_t$ respectively. Given a kernel $K$ and its corresponding feature space $\\mathcal{H}$, we define the MMD as:\n",
    "\\begin{equation}\n",
    "MMD \\left[X_s, X_t\\right] \\ = \\\n",
    "Tr \\left(KL\\right)\n",
    "\\quad \\text{with} \\quad\n",
    "K = \\left[\n",
    "\\begin{aligned}\n",
    "&K_s \\ &K_{st} \\\\\n",
    "&K_{ts} \\ &K_t\n",
    "\\end{aligned}\n",
    "\\right]\n",
    "\\ \\text{and} \\\n",
    "L = \\left[\n",
    "\\begin{aligned}\n",
    "&\\frac{1}{n_s^2} \\ &-\\frac{1}{n_sn_t}\\\\\n",
    "&-\\frac{1}{n_sn_t} \\ &\\frac{1}{n_t^2}\\\\\n",
    "\\end{aligned}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import kernel_metrics\n",
    "\n",
    "def create_sub_L(X, Y):\n",
    "    l = np.ones((X.shape[0], Y.shape[0]))\n",
    "    return l / l.shape[0] / l.shape[1]\n",
    "\n",
    "def compute_MMD(X_source, X_target, kernel_name, kernel_params):\n",
    "    kernel = kernel_metrics()[kernel_name]\n",
    "    \n",
    "    K_s = kernel(X_source, **kernel_params)\n",
    "    K_st = kernel(X_source, X_target, **kernel_params)\n",
    "    K_t = kernel(X_target, **kernel_params)\n",
    "    K = np.block([[K_s, K_st], [K_st.T, K_t]])\n",
    "    L = np.block([\n",
    "        [create_sub_L(X_source, X_source), - create_sub_L(X_source, X_target)],\n",
    "        [-create_sub_L(X_target, X_source), create_sub_L(X_target, X_target)]\n",
    "    ])\n",
    "    \n",
    "    return np.trace(K.dot(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several questions using the MMD:\n",
    "<ul>\n",
    "    <li> What happens if you first project data on a few gene combinations ? Is there a direction that minimize the MMD between source and target ?\n",
    "    <li> Can we extend Kernel PCA using MMD ?\n",
    "    <li> What would be the effect of the kernel choice on the computation of the MMD ?\n",
    "    <li> How do we scale the computation of MMD between large samples ?\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we did not touch upon\n",
    "We showed an example of domain adaptation applied to genomics. There are various other applications and transfer learning methods that exist out there. A few interesting examples:\n",
    "<ul>\n",
    "    <li> <b>Neural network pre-training</b>: Once a neural network has been trained on a source dataset, it can be interesting to tune it on the target dataset. This only applies if some labelled data is available on the test set.\n",
    "    <li> <b>Sample reweighting</b>: Another strategy consists in weighting source samples according to how well they fit inside the target data. For instance, a source sample with a lot of close target samples will be down-weighted -- in the meantime, a source samples without any close correspondance in the target data will be down-weighted.\n",
    "    <li> <b> Distribution comparisons:</b> We presented here an instance of <em>subspace-based</em> transfer learning. Another interesting avenue to domain adaptation is to compare distributions directly, for instance using Maximum Mean Discrepancy.\n",
    "    <li> <b>Domain adaptation theory</b>: Understanding the mechanisms of generalization is an important topic. This directly applies to domain adaptation and transfer learning: are there any conditions under which domain adaptation provably works? Although there is no clear answers, some interesting theoretical work has been done (see [Ben-David et al 2010]).\n",
    "    <li> <b>Zero-shot learning and meta-learning</b>: An extreme case of domain adaptation is zero-shot learning. In this scenario, not only does the test set differ from the training set, but some target classes are not known.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- <a href=\"https://ieeexplore.ieee.org/abstract/document/8861136\">A review of domain adaptation without target labels </a>, Kouw et al 2019, TPAMI. \n",
    "- <a href=\"https://www.biorxiv.org/content/10.1101/2020.06.29.177139v3\">Predicting clinical drug response from model systems by non-linear subspace-based transfer learning</a>, Mourragui et al 2020, Biorxiv\n",
    "- <a href=\"https://www.jmlr.org/papers/volume13/gretton12a/gretton12a.pdf\">A kernel two-sample test</a>, Gretton et al 2012, Journal of Machine Learning Research"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oncode_mc",
   "language": "python",
   "name": "oncode_mc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
